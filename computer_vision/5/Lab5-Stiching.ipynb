{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Image Stitching for Panoramas\n",
    "\n",
    "For this assignment, you will be writing a program that creates an image panorama from 3 images.  In general, this technique should be applicable to any number of photographs.  The approach described below will work well for collective fields of view up to 90&deg; or even 120&deg;, but won't produce ideal results for large fields of view approaching or surpassing 180&deg;.  For large fields of view, cylindrical or spherical projection is required.\n",
    "\n",
    "When we construct a panorama, we assume that all of the photographs were taken from the exact same location and that the images are related by pure rotation (no translation of the camera).  The easiest way to create the panorama is to project all of the photos onto a plane.  One photo must be selected to be the base/center photo.  The other photos are aligned to this base photo by identifying a homography (a planar warp specified by 4 pairs of source/destination points) relating each pair.  Each of the other images is appropriately warped and composited onto the plane (the base image doesnâ€™t need to be warped).\n",
    "\n",
    "In this lab, you will be testing your setup for the following three images:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "left = cv2.imread(\"im1_left.jpg\")\n",
    "center = cv2.imread(\"im1_center.jpg\")\n",
    "right = cv2.imread(\"im1_right.jpg\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 10))\n",
    "axes[0].imshow(left[:,:,::-1])\n",
    "axes[1].imshow(center[:,:,::-1])\n",
    "axes[2].imshow(right[:,:,::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Finding Interest Points/Descriptors\n",
    "We will be using OpenCV for this project, which you should already have installed. Although the SIFT algorithm is patented and used to require a separate install, it is now included in the newer versions of OpenCV. A good tutorial on how to use SIFT features in OpenCV is found [here](https://docs.opencv.org/trunk/da/df5/tutorial_py_sift_intro.html).  The first step to registering or aligning two or more images is to identify locations in each image that are distinctive or stand out.  The `sift.detectAndCompute()` routine produces both these interest points and their corresponding SIFT descriptors.  The first step of producing a panorama is to load all of the relevant images and find the interest points and their descriptors.\n",
    "\n",
    "For this part:\n",
    "* Create a function called `getKeypoints` that takes in an image, converts it to grayscale, computes its keypoints and descriptions, and returns them.\n",
    "* Output a visual showing the keypoints for all three images using OpenCV's `drawKeypoints` function. Make sure to use the `flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS` argument, which scales the circle based on the scale of each keypoint. An example is provided in the examples document on Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKeypoints(image):\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    # NOTE: keypoints have an (x, y), scale, orientation, response (strength of feature), and octave (layer of the scale space where it was found)\n",
    "    # NOTE: descriptors are 128 element vector describing the neighborhood of the corresponding keypoint\n",
    "    \n",
    "    keypoints, descriptors = sift.detectAndCompute(image = gray, mask = None)\n",
    "\n",
    "    return keypoints, descriptors, gray\n",
    "\n",
    "\n",
    "\n",
    "def draw_kp(images, titles):\n",
    "\n",
    "    plt.figure(figsize = (10, 10))\n",
    "\n",
    "    for i, (image, title) in enumerate(zip(images, titles)):\n",
    "        kp, _, gray = getKeypoints(image)\n",
    "\n",
    "        canvas = cv2.drawKeypoints(\n",
    "            image = gray, \n",
    "            keypoints = kp, \n",
    "            outImage = None, \n",
    "            flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
    "        )\n",
    "\n",
    "        plt.subplot(1, len(images), i + 1) # 1 row, num of images as columns, current position\n",
    "        plt.imshow(canvas, cmap = 'gray')\n",
    "        plt.title(f'{title}')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [left, center, right]\n",
    "titles = ['left', 'center', 'right']\n",
    "\n",
    "draw_kp(images, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Matching Features\n",
    "\n",
    "Next, given the features present in each image, you need to match the features so as to determine corresponding points between adjacent/overlapping images.  [This page](https://docs.opencv.org/trunk/dc/dc3/tutorial_py_matcher.html) provides details to do feature matching using `cv2.BFMatcher()`, analogous to the approach proposed by David Lowe in his original implementation.  Be aware that the resulting match is one directional. \n",
    "\n",
    "For this part:\n",
    "* Create a function called `getMatches` that takes in two sets of SIFT descriptions and returns matches between them. Use `knnMatch` and use a ratio test to eliminate bad matchese as described in [this tutorial](https://docs.opencv.org/trunk/dc/dc3/tutorial_py_matcher.html).\n",
    "* Output a visual showing the matches between left-center and right-center using OpenCV's `drawMatches` or `drawMatchesKNN` function. An example is provided on the Canvas examples document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatches(query_des, train_des, ratio = .75):\n",
    "\n",
    "    # init brute force matcher\n",
    "    bf = cv2.BFMatcher(normType = cv2.NORM_L2, crossCheck = False)\n",
    "    # find matches\n",
    "    matches = bf.knnMatch(queryDescriptors = query_des, trainDescriptors = train_des, k = 2)\n",
    "    return [ m for m, n in matches if m.distance < ratio * n.distance ]\n",
    "\n",
    "def draw_matches(query_image, kpq, train_image, kpt, matches, title):\n",
    "\n",
    "    result = cv2.drawMatches(\n",
    "        img1 = query_image, \n",
    "        keypoints1 = kpq, \n",
    "        img2 = train_image, \n",
    "        keypoints2 = kpt, \n",
    "        matches1to2 = matches, \n",
    "        outImg = None,\n",
    "        flags = cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize = (5, 5))\n",
    "    plt.imshow(result[...,:: -1])\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keypoints and descriptors\n",
    "kp_l, des_l, _ = getKeypoints(left)\n",
    "kp_c, des_c, _ = getKeypoints(center)\n",
    "kp_r, des_r, _ = getKeypoints(right)\n",
    "\n",
    "# Matches\n",
    "matches_lc = getMatches(des_l, des_c) # left and center matches\n",
    "matches_rc = getMatches(des_r, des_c) # center and right\n",
    "\n",
    "# show left-center\n",
    "draw_matches(left, kp_l, center, kp_c, matches_lc, 'Left-Center')\n",
    "# show center-right\n",
    "draw_matches(right, kp_r, center, kp_c, matches_rc, 'Right-Center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Estimating Pairwise Homographies using RANSAC\n",
    "\n",
    "Use the RANSAC algorithm ([Szeliski](http://szeliski.org/Book/), Ch 8.1.4), estimate the homography between each pair of images. For this section, you may use `cv2.findHomography()` and its associated RANSAC implementation. Alternatively, you can use the code from Lab 4 to generate the homography matrix and implement RANSAC yourself.\n",
    "\n",
    "For this part:\n",
    "* Create a function called `getHomography` that takes in two sets of keypoints and a list of matches and returns a best homography using the RANSAC algorithm. \n",
    "* Output a visual showing the warped images after the applied homography using `cv2.warpPerspective()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Too Stubborn to use the given RANSAC functions...`\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Point Class`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Point():\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Build Homography`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_homography(s0, s1, s2, s3, t0, t1, t2, t3):\n",
    "\n",
    "    x0s = s0.x\n",
    "    y0s = s0.y\n",
    "    x0t = t0.x\n",
    "    y0t = t0.y\n",
    "\n",
    "    x1s = s1.x\n",
    "    y1s = s1.y\n",
    "    x1t = t1.x\n",
    "    y1t = t1.y\n",
    "\n",
    "    x2s = s2.x\n",
    "    y2s = s2.y\n",
    "    x2t = t2.x\n",
    "    y2t = t2.y\n",
    "\n",
    "    x3s = s3.x\n",
    "    y3s = s3.y\n",
    "    x3t = t3.x\n",
    "    y3t = t3.y\n",
    "\n",
    "    # linear constraints betwixt source and target\n",
    "    A = np.matrix([\n",
    "            [x0s, y0s, 1, 0, 0, 0, -x0t * x0s, -x0t * y0s],\n",
    "            [0, 0, 0, x0s, y0s, 1, -y0t * x0s, -y0t * y0s],\n",
    "            [x1s, y1s, 1, 0, 0, 0, -x1t * x1s, -x1t * y1s],\n",
    "            [0, 0, 0, x1s, y1s, 1, -y1t * x1s, -y1t * y1s],\n",
    "            [x2s, y2s, 1, 0, 0, 0, -x2t * x2s, -x2t * y2s],\n",
    "            [0, 0, 0, x2s, y2s, 1, -y2t * x2s, -y2t * y2s],\n",
    "            [x3s, y3s, 1, 0, 0, 0, -x3t * x3s, -x3t * y3s],\n",
    "            [0, 0, 0, x3s, y3s, 1, -y3t * x3s, -y3t * y3s]\n",
    "        ])\n",
    "\n",
    "    # column vector of target coords\n",
    "    b = np.array([\n",
    "            [x0t],\n",
    "            [y0t],\n",
    "            [x1t],\n",
    "            [y1t],\n",
    "            [x2t],\n",
    "            [y2t],\n",
    "            [x3t],\n",
    "            [y3t]\n",
    "        ])\n",
    "    \n",
    "    try:\n",
    "        # A^-1 @ b\n",
    "        solutions = np.linalg.solve(A, b)\n",
    "    except np.linalg.LinAlgError:\n",
    "        # the matrix isn't invertible\n",
    "        A += np.eye(A.shape[0]) * 1e-10 # add SMALL value to matrix to make it invertible\n",
    "        solutions = np.linalg.solve(A, b)\n",
    "\n",
    "    # homogenize the matrix\n",
    "    solutions = np.append(solutions, [[1.0]], axis = 0)\n",
    "\n",
    "    # reshape homography to 3x3 matrix\n",
    "    homography = np.reshape(solutions, (3,3))\n",
    "    \n",
    "    return homography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Custom RANSAC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ransac_homography(kp_q, kp_t, matches, max = 1000, threshold = 5.0):\n",
    "    import random\n",
    "\n",
    "    best_H = None\n",
    "    max_inliers = 0\n",
    "    best_inliers = []\n",
    "\n",
    "    for _ in range(max):\n",
    "        samples = random.sample(matches, 4)\n",
    "\n",
    "        query_points = [Point(*kp_q[m.queryIdx].pt) for m in samples]\n",
    "        train_points = [Point(*kp_t[m.trainIdx].pt) for m in samples]\n",
    "\n",
    "        H = get_homography(*query_points, *train_points)\n",
    "\n",
    "        inliers = []\n",
    "\n",
    "        for m in matches:\n",
    "\n",
    "            pt1 = np.array([*kp_q[m.queryIdx].pt, 1.0]) # homogenized coords\n",
    "            pt2 = np.array(kp_t[m.trainIdx].pt)\n",
    "\n",
    "            projected_pt = H @ pt1\n",
    "            projected_pt /= projected_pt[2] # normalize to grab (x, y)\n",
    "\n",
    "            # euclidean distance as the reprojection error\n",
    "            error = np.linalg.norm(pt2 - projected_pt[:2])\n",
    "\n",
    "            # check if distance is w/in threshold\n",
    "            if error < threshold:\n",
    "                inliers.append(m)\n",
    "\n",
    "            # update best homography\n",
    "            if len(inliers) > max_inliers:\n",
    "                max_inliers = len(inliers)\n",
    "                best_H = H\n",
    "                best_inliers = inliers\n",
    "            \n",
    "    return best_H, best_inliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Bilinear interpolation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(image: np.ndarray, x: float, y: float) -> float | int:\n",
    "    \n",
    "    h, w, _ = image.shape\n",
    "\n",
    "    # get neighboring points\n",
    "    # clip to w/in bounds if necessary\n",
    "    x0 = max(int(np.floor(x)), 0)\n",
    "    y0 = max(int(np.floor(y)), 0)\n",
    "    x1 = min(x0 + 1, w - 1) \n",
    "    y1 = min(y0 + 1, h - 1)\n",
    "\n",
    "    # distances/weights between TL pixel and orginal pixel\n",
    "    xw = x - x0\n",
    "    yw = y - y0\n",
    "\n",
    "    # neighboring pixel values\n",
    "    p00 = image[y0, x0].astype(np.float32) # TL\n",
    "    p01 = image[y0, x1].astype(np.float32) # TR\n",
    "    p10 = image[y1, x0].astype(np.float32) # BL\n",
    "    p11 = image[y1, x1].astype(np.float32) # BR\n",
    "\n",
    "    return ( \n",
    "        p00 * (1 - xw) * (1 - yw) + \n",
    "        p01 * xw * (1 - yw) + \n",
    "        p10 * (1 - xw) * yw + \n",
    "        p11 * xw * yw\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Compose function aka perspective warp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose(source: np.ndarray, target: np.ndarray, transform: np.ndarray) -> np.ndarray:\n",
    "    \n",
    "    # dimension grabbing\n",
    "    h, w, _ = source.shape\n",
    "    target_h, target_w, _ = target.shape\n",
    "\n",
    "    result = np.copy(target)\n",
    "    \n",
    "    # invert transformation matrix for backward mapping\n",
    "    transform_inv = np.linalg.inv(transform)\n",
    "\n",
    "    for y in range(target_h):\n",
    "        for x in range(target_w):\n",
    "            \n",
    "            # homogenize\n",
    "            # aka add one dimension\n",
    "            xy_target_homogenized = np.array([x, y, 1]) # (x, y) to (x, y, 1)\n",
    "\n",
    "            # backward mapping\n",
    "            xy_backward_map = transform_inv @ xy_target_homogenized.T # convert to column vector by using \"T\"\n",
    "            xy_backward_map = np.array(xy_backward_map).flatten() # convert to 1d array\n",
    "\n",
    "            # homogenous divide to get source coordinates\n",
    "            x_source = xy_backward_map[0] / xy_backward_map[2]\n",
    "            y_source  = xy_backward_map[1] / xy_backward_map[2]\n",
    "\n",
    "            # just another classic boundary check\n",
    "            if 0 <= x_source < w and 0 <= y_source < h: # we can siimply discard anything not in bounds\n",
    "\n",
    "                result[y, x] = interpolate(source, x_source, y_source)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_left, inliers = ransac_homography(kp_l, kp_c, matches_lc)\n",
    "stitched_lc = compose(left, center, H_left)\n",
    "\n",
    "H_right, inliers = ransac_homography(kp_r, kp_c, matches_rc)\n",
    "stitched_rc = compose(right, center, H_right)\n",
    "\n",
    "_, axes = plt.subplots(nrows = 1, ncols = 2, figsize=(10, 10))\n",
    "axes[0].imshow(stitched_lc[...,:: -1]), axes[0].set_title('Left - Center')\n",
    "axes[1].imshow(stitched_rc[...,:: -1]), axes[1].set_title('Center - Right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D: Creating the Mosaic\n",
    "\n",
    "When you warped the images in Part C, you may have noticed that `cv2.warpPerspective()` cuts off the part of the image that doesn't fit in the frame. To fix this, we will need to create a larger canvas image and then map the three images into this space to create the final panorama.\n",
    "\n",
    "To create the final panorama:\n",
    "* Create a canvas image that is 3 times larger in height and width than the center image.\n",
    "* Create a translation matrix that accounts for this padding, where tx = columns and ty = rows.\n",
    "* Multiply the two homography matrices by this translation matrix.\n",
    "* Use `cv2.warpPerspective()` to map the left and right images onto the canvas. **Make sure the destination size is the canvas shape.**\n",
    "* Place the center image in the middle of the canvas, overwriting any values from the left or right images.\n",
    "* Crop the compositied image to the smallest bounding box, removing any remaining black border (`np.nonzero` may be helpful)\n",
    "\n",
    "An example of this very simple (but not ideal) compositing operation is given in the examples document on Canvas. **It is okay if your result has noticable seams between the three images.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(image):\n",
    "    \n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    nonzero = np.nonzero(gray)\n",
    "\n",
    "    y_min, y_max = nonzero[0].min(), nonzero[0].max()\n",
    "    x_min, x_max = nonzero[1].min(), nonzero[1].max()\n",
    "\n",
    "    return image[y_min : y_max, x_min : x_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blendImages(left, center, right, H_left, H_right):\n",
    "\n",
    "    h, w, _ = center.shape\n",
    "    \n",
    "    canvas = np.zeros((3 * h, 3 * w, 3), dtype = np.uint8)\n",
    "    y = (canvas.shape[0] - h) // 2\n",
    "    x = (canvas.shape[1] - w) // 2\n",
    "\n",
    "    T = np.array([\n",
    "        [1, 0, w],\n",
    "        [0, 1, h],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    left_transform = T @ H_left\n",
    "    right_transform = T @ H_right\n",
    "\n",
    "    canvas = compose(left, np.copy(canvas), left_transform)\n",
    "    canvas[ y : y + h, x : x + w] = center\n",
    "    canvas = compose(right, np.copy(canvas), right_transform)\n",
    "\n",
    "    return crop(canvas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panorama = blendImages(left, center, right, H_left, H_right)\n",
    "\n",
    "plt.imshow(panorama[..., ::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E: Putting It All Together\n",
    "\n",
    "You now have all needed components to create a mosaic out of three images. For this last part, combine/call all of your previous functions together into a single stitching function that takes three images as input and output the panorama image.\n",
    "\n",
    "You can test your function on the two provided test cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageStitch(left, center, right):\n",
    "\n",
    "    ''' keypoints '''\n",
    "    kp_left, des_left, _ = getKeypoints(left)\n",
    "    kp_center, des_center, _ = getKeypoints(center)\n",
    "    kp_right, des_right, _ = getKeypoints(right)\n",
    "\n",
    "    ''' matches '''\n",
    "    left_center_matches = getMatches(des_left, des_center)\n",
    "    right_center_matches = getMatches(des_right, des_center)\n",
    "\n",
    "    ''' RANSAC '''\n",
    "    H_left, _ = ransac_homography(kp_left, kp_center, left_center_matches)\n",
    "    H_right, _ = ransac_homography(kp_right, kp_center, right_center_matches)\n",
    "\n",
    "    ''' stitch '''\n",
    "    panorama = blendImages(left, center, right, H_left, H_right)\n",
    "\n",
    "    return panorama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = cv2.imread(\"im1_left.jpg\")\n",
    "center = cv2.imread(\"im1_center.jpg\")\n",
    "right = cv2.imread(\"im1_right.jpg\")\n",
    "\n",
    "result = imageStitch(left,center,right)\n",
    "plt.imshow(result[:,:,::-1]);plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = cv2.imread(\"im2_left.jpg\")\n",
    "center = cv2.imread(\"im2_center.jpg\")\n",
    "right = cv2.imread(\"im2_right.jpg\")\n",
    "\n",
    "result = imageStitch(left,center,right)\n",
    "plt.imshow(result[:,:,::-1]);plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Extensions for Extra Credit (5 pts)\n",
    "Once you have completed your fully functional image stitcher, there are many improvements you can make on this process. For 5 pts extra credit, you can implement one of the following:\n",
    "* Allow for larger images to be stitched efficiently/effectively by shrinking the image down to a standard small size for computing keypoints, then mapping those keypoints locations back onto the larger size image before computing the homography.\n",
    "* Automatically detecting which image is the center by computing matches between all pairs of images, then using number of good matches as an indicator of side-by-side images.\n",
    "* Improving the blending operation by using alpha masking and a composition technique such as averaging, Voronoi, or gradient-domain blending.\n",
    "* Allow for more than three and overlapping images to be stitched together.\n",
    "\n",
    "You may need to provide your own images to test these extensions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
