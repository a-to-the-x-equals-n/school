{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Warping Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Interpolation\n",
    "\n",
    "Since we are going to be warping images, we need to make sure we have a good interpolation function to deal with inexact pixel locations.\n",
    "\n",
    "In the cell below, we want to scale an image by a factor of 2.30 in both x and y. **Write a function called \"interpolate\" that performs bilinear interpolation.** This function should take in the image and a location (x,y) of interest. It then uses the nearby pixels to interpolate and return an RGB value.\n",
    "\n",
    "**Note:** For this lab, we will keep images as floats to simplify the interpolation. Thus, output your images as values between 0-1, not 0-255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#Your bilinear interpolation function\n",
    "def interpolate(image: np.ndarray, x: float, y: float) -> float | int:\n",
    "    \n",
    "    h, w, _ = image.shape\n",
    "\n",
    "    # get neighboring points\n",
    "    # clip to w/in bounds if necessary\n",
    "    x0 = max(int(np.floor(x)), 0)\n",
    "    y0 = max(int(np.floor(y)), 0)\n",
    "    x1 = min(x0 + 1, w - 1) \n",
    "    y1 = min(y0 + 1, h - 1)\n",
    "\n",
    "    # distances/weights between TL pixel and orginal pixel\n",
    "    xw = x - x0\n",
    "    yw = y - y0\n",
    "\n",
    "    # neighboring pixel values\n",
    "    p00 = image[y0, x0].astype(np.float32) # TL\n",
    "    p01 = image[y0, x1].astype(np.float32) # TR\n",
    "    p10 = image[y1, x0].astype(np.float32) # BL\n",
    "    p11 = image[y1, x1].astype(np.float32) # BR\n",
    "\n",
    "    return ( \n",
    "        p00 * (1 - xw) * (1 - yw) + \n",
    "        p01 * xw * (1 - yw) + \n",
    "        p10 * (1 - xw) * yw + \n",
    "        p11 * xw * yw\n",
    "    )\n",
    "\n",
    "filename = \"test.png\"\n",
    "im = plt.imread(filename)\n",
    "\n",
    "plt.imshow(im,vmin=0)\n",
    "plt.show()\n",
    "\n",
    "print(np.amax(im)) #Image values should be between 0-1, not 0-255\n",
    "\n",
    "h, w, _ = im.shape\n",
    "\n",
    "result = np.zeros((int(2.3 * h), (int(2.3 * w)), 3), dtype = \"float32\")\n",
    "\n",
    "rows, cols, _ = result.shape\n",
    "\n",
    "# Map each new point back to the original image\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        result[i,j] = interpolate(im, j / 2.3, i / 2.3)\n",
    " \n",
    "\n",
    "plt.imshow(result,vmin=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Compose (i.e. Backwards Mapping)\n",
    "\n",
    "Now that we have a interpolation function, we need a function that can compose a source image onto a target image given a transform. In Lab 3, we used Pillow's built-in compose function. You will now implement this yourself using backwards mapping. **This function should also call your interpolate function.**\n",
    "\n",
    "To implement this function,\n",
    "- Invert the given transformation. (This is really easy using the NumPy *linalg* library)\n",
    "- For each pixel in the target image\n",
    "  - Determine the x and y coordinates of the pixel\n",
    "  - Homogenize the coordinate\n",
    "  - Multiply it by the inverse transform\n",
    "  - Perform the homogenous divide on the new point\n",
    "  - If the x,y coordinate is within the boundaries of the source image, grab the interpolated value from source image. Otherwise, keep the original pixel in the target image.\n",
    "\n",
    "For this example, a simple rotation transformation will be given. The source and target image will be the same size, which means part of your rotated image will be cut off on the corners. If done correctly, you should see the parrot rotated onto a purple background. You can assume that all pixels need to be backward mapped. Any pixel/point that maps to a point outside the image boundaries should be skipped. **Don't forget the homogenous divide.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose(source: np.ndarray, target: np.ndarray, transform: np.ndarray) -> np.ndarray:\n",
    "    \n",
    "    # dimension grabbing\n",
    "    h, w, _ = source.shape\n",
    "    target_h, target_w, _ = target.shape\n",
    "\n",
    "    result = np.copy(target)\n",
    "    \n",
    "    # invert transformation matrix for backward mapping\n",
    "    transform_inv = np.linalg.inv(transform)\n",
    "\n",
    "    for y in range(target_h):\n",
    "        for x in range(target_w):\n",
    "\n",
    "            # homogenize\n",
    "            # aka add one dimension\n",
    "            xy_target_homogenized = np.array([x, y, 1]) # (x, y) to (x, y, 1)\n",
    "\n",
    "            # backward mapping\n",
    "            xy_backward_map = transform_inv @ xy_target_homogenized.T # convert to column vector by using \"T\"\n",
    "            xy_backward_map = np.array(xy_backward_map).flatten() # convert to 1d array\n",
    "\n",
    "            # homogenous divide to get source coordinates\n",
    "            x_source = xy_backward_map[0] / xy_backward_map[2]\n",
    "            y_source  = xy_backward_map[1] / xy_backward_map[2]\n",
    "\n",
    "            # just another classic boundary check\n",
    "            if 0 <= x_source < w and 0 <= y_source < h: # we can siimply discard anything not in bounds\n",
    "\n",
    "                # NOTE FOR FUTURE LOGAN\n",
    "                ''' \n",
    "                    We multiplied the inverse of the transformation matrix with a homogenized x and y from the target image.\n",
    "                    \n",
    "                    That process is a \"map\" to find the exact x and y coords from a source.\n",
    "                    \n",
    "                        Now that we've found the position of WHERE the source image's x and y are...\n",
    "\n",
    "                        ...we interpolate them to grab an estimated pixel color from that location.\n",
    "                    \n",
    "                    Once we have the pixel, we set the current x and y coordinate of the target to that pixel.\n",
    "                '''\n",
    "                result[y, x] = interpolate(source, x_source, y_source)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "from math import sin, cos, pi\n",
    "\n",
    "filename = \"test.png\"\n",
    "im = plt.imread(filename)\n",
    "canvas = np.zeros_like(im) + np.array([[[ 80/255, 45/255, 127/255 ]]]) #ECU Purple\n",
    "\n",
    "transform = np.matrix([[cos(45 * pi/180), -sin(45 * pi/180), im.shape[1]/2],\n",
    "                       [sin(45 * pi/180), cos(45 * pi/180), -im.shape[0]/5],\n",
    "                       [0, 0, 1]])\n",
    "\n",
    "result = compose(im, canvas, transform)\n",
    "\n",
    "plt.imshow(result, vmin = 0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Perspective Warp\n",
    "\n",
    "Now that we have the two specific functions that we need, let's start looking at some more interesting image warping. In class, we discussed how we can use homographies to perform perspective warps. In this lab, we have provided the homography generating code for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Point():\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"({},{})\".format(self.x,self.y)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "def getHomography(s0, s1, s2, s3, t0, t1, t2, t3):\n",
    "\n",
    "    x0s = s0.x\n",
    "    y0s = s0.y\n",
    "    x0t = t0.x\n",
    "    y0t = t0.y\n",
    "\n",
    "    x1s = s1.x\n",
    "    y1s = s1.y\n",
    "    x1t = t1.x\n",
    "    y1t = t1.y\n",
    "\n",
    "    x2s = s2.x\n",
    "    y2s = s2.y\n",
    "    x2t = t2.x\n",
    "    y2t = t2.y\n",
    "\n",
    "    x3s = s3.x\n",
    "    y3s = s3.y\n",
    "    x3t = t3.x\n",
    "    y3t = t3.y\n",
    "\n",
    "    #Solve for the homography matrix\n",
    "    A = np.matrix([\n",
    "            [x0s, y0s, 1, 0, 0, 0, -x0t*x0s, -x0t*y0s],\n",
    "            [0, 0, 0, x0s, y0s, 1, -y0t*x0s, -y0t*y0s],\n",
    "            [x1s, y1s, 1, 0, 0, 0, -x1t*x1s, -x1t*y1s],\n",
    "            [0, 0, 0, x1s, y1s, 1, -y1t*x1s, -y1t*y1s],\n",
    "            [x2s, y2s, 1, 0, 0, 0, -x2t*x2s, -x2t*y2s],\n",
    "            [0, 0, 0, x2s, y2s, 1, -y2t*x2s, -y2t*y2s],\n",
    "            [x3s, y3s, 1, 0, 0, 0, -x3t*x3s, -x3t*y3s],\n",
    "            [0, 0, 0, x3s, y3s, 1, -y3t*x3s, -y3t*y3s]\n",
    "        ])\n",
    "\n",
    "    b = np.matrix([\n",
    "            [x0t],\n",
    "            [y0t],\n",
    "            [x1t],\n",
    "            [y1t],\n",
    "            [x2t],\n",
    "            [y2t],\n",
    "            [x3t],\n",
    "            [y3t]\n",
    "        ])\n",
    "\n",
    "    #The homorgraphy solutions a-h\n",
    "    solutions = np.linalg.solve(A,b)\n",
    "\n",
    "    solutions = np.append(solutions,[[1.0]], axis=0)\n",
    "\n",
    "    #Reshape the homography into the appropriate 3x3 matrix\n",
    "    homography = np.reshape(solutions, (3,3))\n",
    "    \n",
    "    return homography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to be able to get a new image into the TV set in the image shown below. \n",
    "\n",
    "To place the bird correctly inside the TV, do the following:\n",
    "\n",
    "- Determine the homography that maps from the bird image to the TV. Use the `getHomography` function and the provided points.\n",
    "\n",
    "- For each pixel/point within the target image, backmap those points onto the bird image. Use your `compose` function and make sure it correctly rejects points that do not land within the image boundary\n",
    "\n",
    "Write a general perspective warp function that completes these steps. **Hint: If your compose/backmap function is written correctly, this should only be 2-3 lines of code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perspectiveWarp(source_im ,target_im, s0, s1, s2, s3, t0, t1, t2, t3):\n",
    "    '''\n",
    "        NOTE Dear future Logan,\n",
    "\n",
    "            Remember, the homography matrix (in this context) is just the transformation matrix.\n",
    "\n",
    "            If you have all 8 points -- 4 from the source, and 4 from the destination -- the homography \n",
    "            matrix describes the relationship of change between the two images.\n",
    "\n",
    "            In this example, we just took the corner coordinates of the source image, and then the \n",
    "            four coordinates of the destination to map the original image to the new location.\n",
    "    '''\n",
    "    return compose(source_im, (result := np.copy(target_im)), getHomography(s0, s1, s2, s3, t0, t1, t2, t3))\n",
    "    \n",
    "\n",
    "filename = \"test.png\"\n",
    "im = plt.imread(filename)\n",
    "\n",
    "h,w,_ = im.shape\n",
    "        \n",
    "s0 = Point(0,0)\n",
    "s1 = Point(w-1,0)\n",
    "s2 = Point(w-1,h-1)\n",
    "s3 = Point(0,h-1)\n",
    "\n",
    "t0 = Point(245,152)\n",
    "t1 = Point(349,150)\n",
    "t2 = Point(349,253)\n",
    "t3 = Point(246,261)\n",
    "\n",
    "tv = plt.imread('tv.png')\n",
    "plt.imshow(tv,vmin=0)\n",
    "plt.show()\n",
    "\n",
    "result = perspectiveWarp(im,tv,s0,s1,s2,s3,t0,t1,t2,t3)\n",
    "\n",
    "plt.imshow(result,vmin=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Speed Up\n",
    "\n",
    "We have a complete perspective warping algorithm now, but you may have noticed that it runs a little slowly. This is because it checks every pixel in the target image to see if it needs to be backmapped. A much faster solution is create a min-max bounding box around the warping points and only checking pixels within that bounding box. **See the bounding box example image provided with the lab.** You may find `np.amax()` and `np.amin()` to be helpful.\n",
    "\n",
    "For this last part of the lab, rewrite your perspective warp and compose functions as needed so that they only check pixels in the relevant bounding box in the image. Compare the run times between the previous approach and your new method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def composeFast(source, target, transform, box = None):   \n",
    "    \n",
    "    # dimension grabbing\n",
    "    h, w, _ = source.shape\n",
    "\n",
    "    result = np.copy(target)\n",
    "    \n",
    "    # invert transformation matrix for backward mapping\n",
    "    transform_inv = np.linalg.inv(transform)\n",
    "\n",
    "    for y in range(box[0].y, box[1].y + 1):\n",
    "        for x in range(box[0].x, box[1].x + 1):\n",
    "\n",
    "            # homogenize\n",
    "            # aka add one dimension\n",
    "            xy_target_homogenized = np.array([x, y, 1]) # (x, y) to (x, y, 1)\n",
    "\n",
    "            # backward mapping\n",
    "            xy_backward_map = transform_inv @ xy_target_homogenized.T # convert to column vector by using \"T\"\n",
    "            xy_backward_map = np.array(xy_backward_map).flatten() # convert to 1d array\n",
    "\n",
    "            # homogenous divide to get source coordinates\n",
    "            x_source = xy_backward_map[0] / xy_backward_map[2]\n",
    "            y_source  = xy_backward_map[1] / xy_backward_map[2]\n",
    "\n",
    "            # just another classic boundary check\n",
    "            if 0 <= x_source < w and 0 <= y_source < h: # we can siimply discard anything not in bounds\n",
    "                result[y, x] = interpolate(source, x_source, y_source)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def perspectiveWarpFast(source_im,target_im,s0,s1,s2,s3,t0,t1,t2,t3):\n",
    "\n",
    "    start = Point(np.amin([t0.x, t1.x, t2.x, t3.x]),\n",
    "                  np.amin([t0.y, t1.y, t2.y, t3.y]))\n",
    "    \n",
    "    end = Point(np.amax([t0.x, t1.x, t2.x, t3.x]), \n",
    "                np.amax([t0.y, t1.y, t2.y, t3.y]))\n",
    "    \n",
    "    return composeFast(source_im, (result := np.copy(target_im)), getHomography(s0, s1, s2, s3, t0, t1, t2, t3), (start, end))\n",
    "    \n",
    "\n",
    "from time import time\n",
    "    \n",
    "museum = plt.imread('museum.png')\n",
    "plt.imshow(museum,vmin=0)\n",
    "plt.show()\n",
    "\n",
    "filename = \"test.png\"\n",
    "im = plt.imread(filename)\n",
    "\n",
    "h,w,_ = im.shape\n",
    "        \n",
    "s0 = Point(0,0)\n",
    "s1 = Point(w-1,0)\n",
    "s2 = Point(w-1,h-1)\n",
    "s3 = Point(0,h-1)\n",
    "\n",
    "t0 = Point(268,230)\n",
    "t1 = Point(349,249)\n",
    "t2 = Point(347,361)\n",
    "t3 = Point(267,363)\n",
    "\n",
    "#Points for nearest canvas\n",
    "#t0 = Point(0,170)\n",
    "#t1 = Point(235,222)\n",
    "#t2 = Point(236,365)\n",
    "#t3 = Point(0,368)\n",
    "\n",
    "\n",
    "start = time()\n",
    "result1 = perspectiveWarp(im,museum,s0,s1,s2,s3,t0,t1,t2,t3)\n",
    "print(\"Slow Method:\",time()-start)\n",
    "\n",
    "plt.imshow(result1,vmin=0)\n",
    "plt.show()\n",
    "\n",
    "start = time()\n",
    "result2 = perspectiveWarpFast(im,museum,s0,s1,s2,s3,t0,t1,t2,t3)\n",
    "print(\"Fast Method:\",time()-start)\n",
    "\n",
    "#plt.imsave(\"Output.png\",result2)\n",
    "plt.imshow(result2,vmin=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: QR Code Reader\n",
    "\n",
    "We now have all the basic building blocks to make our own QR code reader. You will implement one for the last part of the lab. We will focus primarily the computer vision side of QR codes, which involves detecting, aligning, and discretizing QR codes so that they can be read properly.\n",
    "\n",
    "### Understanding QR Codes\n",
    "\n",
    "QR codes (short for Quick Response Codes) were invented in 1994, by the DENSO Corporation. These codes store data in two dimensions in the form of an array of contrasting regions. The information density of a QR code is much higher than a vanilla barcode; depending on the format used and the resolution of reader, over a thousand bytes can be encoded in a region the size of a postage stamp.\n",
    "\n",
    "QR codes use a Reed–Solomon error correction based technology to help recover from errors in reading (for instance, caused by a smudge, badly printed code or other deformity).\n",
    "\n",
    "Any QR code can be broken into the following sections:\n",
    "\n",
    "<div style=\"width:60%;text-align:center;\"><img src=\"qr-code-images/formatting.jpg\" width=100%></div>\n",
    "\n",
    "On three corners of a QR code are square blocks that the reader uses to coarsely identify and then align the code. These position markers always have a black/white/black/white/black ratio of 1:1:3:1:1, no matter the angle they are approached from.\n",
    "\n",
    "<div style=\"width:30%;text-align:center;\"><img src=\"qr-code-images/finder_pattern.jpg\" width=100%></div>\n",
    "\n",
    "The fourth point is called the alignment point and has a slightly different pattern and location. This allows the QR code to be properly oriented. Some smaller QR codes do not have this alignment block, so they approximate this location using an affine transform of the three given points.\n",
    "\n",
    "Once these four points are identified, the image can be aligned using a homography and warping. The number of rows and columns in the QR code is determined by the timing information which alternates from black and white in both the vertical and horizontal direction. \n",
    "\n",
    "Once the image is aligned and the size determined, the QR code is discretized, undergoes an Xor with a particular mask given the format information, and read bit for bit in the following order:\n",
    "\n",
    "<div style=\"width:60%;text-align:center;\"><img src=\"qr-code-images/decoding.jpg\" width=100%></div>\n",
    "\n",
    "For more details about the decoding process, see the [DataGenetics Wounded QR Codes Blog](http://datagenetics.com/blog/november12013/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "You will be implementing a QR code reader for 29x29 QR codes. The very first step of identifying the three corners using the 1:1:3:1:1 ratio has been implemented for you. For simplicity, the code does not return a fourth alignment point, but rather returns an axis-aligned point as if a corner was present, as shown in the figure. \n",
    "\n",
    "<div style=\"width:40%;text-align:center;\"><img src=\"qr-code-images/keypoints.png\" width=100%></div>\n",
    "\n",
    "The very last step of taking a NumPy array of 1's and 0's and decoding it using the QR code standard has also been implemented for you. Your task is to implement everything inbetween.\n",
    "\n",
    "To implement the QR code reader for a given input image:\n",
    "- Feed the image into the provided `getCornerPoints` function to the get the x,y coordinates of the four corners.\n",
    "- Use your `perspectiveWarp` to map the four points to an axis-aligned square centered on a 29x29 blank image (sometimes called a buffer or canvas).\n",
    "- Discretize/threshold the image into True/False values.\n",
    "- Feed the 29x29 grid into the provided `decode` function. A string will be returned with the QR code URL data.\n",
    "\n",
    "**Remember that the locations of corners are not on the edge, but 4 pixels in on the 29x29 grid.** Once you have implemented the code, run it on the test cases below.\n",
    "\n",
    "The decoder function depends on packages called `requests` and `BeautifulSoup` to run an online API call. You may need to `pip install` these packages if the decoder doesn't run properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qr import getCornerPoints, decode\n",
    "\n",
    "def check_timing(image: np.ndarray, d: int) -> bool:\n",
    "\n",
    "    # start / stop indices for timing rows and columns\n",
    "    y = 7 - 1 # finder pattern - spacing\n",
    "    x = 7 + 1 # finder pattern + spacing\n",
    "    length = d - 7 - 1 # dimensions - pattern - spacing\n",
    "\n",
    "    timing_row = image[ y, x : length ]\n",
    "    timing_col = image[ x : length, y ]\n",
    "\n",
    "    # timing check\n",
    "    # NOTE white = True, black = False\n",
    "    return (np.all(timing_row[0::2] == False) and \n",
    "            np.all(timing_row[1::2] == True) and\n",
    "            np.all(timing_col[0::2] == False) and\n",
    "            np.all(timing_col[1::2] == True)) \n",
    "\n",
    "\n",
    "def build_qr_grid(d: int):\n",
    "    # dynamically build a 'd x d' canvas\n",
    "    # finder patterns are universal amongst all QR codes\n",
    "    return Point(3, 3), Point(d - 4, 3), Point(3, d - 4), Point(d - 4, d - 4)\n",
    "\n",
    "def scanQRcode(image: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    # RGBA -> RGB\n",
    "    image = image[..., :3] if image.shape[2] == 4 else image\n",
    "\n",
    "    # normalize\n",
    "    # ...just in case\n",
    "    image = image.astype(np.float32) / 255.0 if image.max() > 1.0 else image.astype(np.float32)\n",
    "    \n",
    "    # homography coords\n",
    "    s0, s1, s2, s3 = getCornerPoints(image)\n",
    "    \n",
    "    # loops through grids '21 x 21' - '41 x 41'\n",
    "    # stops iterating and returns QR code that passes timing check\n",
    "    for d in range(21, 42, 4):\n",
    "\n",
    "        # other homography coords\n",
    "        t0, t1, t2, t3 = build_qr_grid(d) \n",
    "        target = np.zeros(shape = (d, d, 3), dtype = np.float32) # I don't want to rework my other functions, so we're initializing this baby with 3 color channels for absolutely no reason\n",
    "    \n",
    "        # warp image -> gray image -> binarize image\n",
    "        warped = perspectiveWarp(image, target, s0, s1, s2, s3, t0, t1, t2, t3)\n",
    "        grayed = warped @ [0.2989, 0.5870, 0.1140]\n",
    "        binarized = grayed > .6\n",
    "        \n",
    "        if check_timing(binarized, d):\n",
    "            plt.imshow(binarized, cmap = 'gray')\n",
    "            plt.title(f'QR Code {d}x{d}')\n",
    "            plt.show()\n",
    "\n",
    "            return decode(binarized)\n",
    "        \n",
    "    raise ValueError(\"something's broken in the loop/check_timing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = plt.imread(\"qr-code-images/test1.png\")\n",
    "plt.imshow(image1)\n",
    "plt.title(f'Original QR Code')\n",
    "plt.show()\n",
    "code1 = scanQRcode(image1)\n",
    "print(code1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image2 = plt.imread(\"qr-code-images/test2.png\")\n",
    "plt.imshow(image2);plt.title(f'Original QR Code');plt.show()\n",
    "code2 = scanQRcode(image2)\n",
    "print(code2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Credit: Any Size QR Code Reader (10 pts)\n",
    "\n",
    "The QR code reader you implemented will work for 29x29 QR codes, but QR codes can be many sizes. All QR codes have an odd number of bits per row and column and go up by 4, starting at size 21 (i.e. 21x21, 25x25, 29x29, ...). For 5 points extra credit, implement a QR code reader that works for any size from 21x21 up to 41x41.\n",
    "\n",
    "To check if a QR code matches a given size, discretize the QR code asumming a 21x21 size. Then, determine if the timing information alternates in the appropriate manner (see the Understanding QR Codes for more information). If the timing information is valid, then you can assume that the QR code is the given size. If not, move up to the next size (25x25) and repeat.\n",
    "\n",
    "You can check if your any-size QR code reader is working by running the following test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ec = plt.imread(\"qr-code-images/extra_credit.png\")\n",
    "plt.imshow(image_ec);plt.title(f'Original QR Code');plt.show()\n",
    "code_ec = scanQRcode(image_ec)\n",
    "print(code_ec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
