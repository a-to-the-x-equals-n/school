{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4139cc19-dd23-4ffa-b6a2-a1b70a6daec6",
   "metadata": {},
   "source": [
    "<center><h1>CSCI 4140: Natural Language Processing</h1></center>\n",
    "<center><h1>CSCI/DASC 6040: Computational Analysis of Natural Languages</h1></center>\n",
    "\n",
    "<center><h6>Spring 2025</h6></center>\n",
    "<center><h6>Homework 1 - N-gram models</h6></center>\n",
    "<center><h6>Due Sunday, January 26, at 11:59 PM</h6></center>\n",
    "\n",
    "<center><font color='red'>Do not redistribute without the instructorâ€™s written permission.</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166d5179-89ef-4186-9af3-ce63ff95cc8b",
   "metadata": {},
   "source": [
    "The learning goals of this assignment are to:\n",
    "\n",
    "- Understand how to compute language model probabilities using maximum likelihood estimation.\n",
    "- Implement back-off.\n",
    "- Have fun using a language model to probabilistically generate texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40408d08-1f30-4286-be61-ac741db46121",
   "metadata": {},
   "source": [
    "# N-gram Language model\n",
    "- For undergraduates: **100 pts + 10 extra credit pts**\n",
    "- For graduates: **110 pts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f41ccf-d54b-4f5d-bd2d-4c8811cd84d1",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84910e85-68eb-41ab-996f-65cad5d8071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import log, exp\n",
    "from collections import defaultdict, Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02bac8f-9980-4905-8029-1d48434008cd",
   "metadata": {},
   "source": [
    "We'll start by loading the data. The [WikiText language modeling dataset](https://huggingface.co/datasets/Salesforce/wikitext) is a collection of tokens extracted from the set of verified Good and Featured articles on Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22459dd5-ddf4-4e53-9e4a-570646a15820",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = {'test': '', 'train': '', 'valid': ''}\n",
    "\n",
    "for data_split in DATA:\n",
    "    fname = \"tokens/wiki.{}.tokens\".format(data_split)\n",
    "    with open(fname, 'r') as f_wiki:\n",
    "        DATA[data_split] = f_wiki.read().lower().split()\n",
    "\n",
    "VOCAB = list(set(DATA['train']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38de2aac-d6c2-4ec2-b0d1-5e2305949705",
   "metadata": {},
   "source": [
    "Now have a look at the data by running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d733449a-e810-4cd1-a03d-e309d41e1dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "global DATA, VOCAB\n",
    "\n",
    "print('train : %s ...' % DATA['train'][:10])\n",
    "print('dev : %s ...' % DATA['valid'][:10])\n",
    "print('test : %s ...' % DATA['test'][:10])\n",
    "print('first 10 words in vocab: %s' % VOCAB[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872e25cc-9b86-408a-b6f0-1fb139effc34",
   "metadata": {},
   "source": [
    "## Q1: Train N-gram language model (60 pts)\n",
    "\n",
    "Complete the following `train_ngram_lm` function based on the following input/output specifications. If you've done it right, you should pass the tests in the cell below.\n",
    "\n",
    "*Input:*\n",
    "+ **data**: the data object created in the cell above that holds the tokenized Wikitext data\n",
    "+ **order**: the order of the model (i.e., the \"n\" in \"n-gram\" model). If order=3, we compute $p(w_2 | w_0, w_1)$.\n",
    "\n",
    "*Output:*\n",
    "+ **lm**: A dictionary where the key is the history and the value is a probability distribution over the next word computed using the maximum likelihood estimate from the training data. Importantly, this dictionary should include *backoff* probabilities as well; e.g., for order=4, we want to store $p(w_3 | w_0,w_1,w_2)$ as well as $p(w_3|w_1,w_2)$ and $p(w_3|w_2)$. \n",
    "\n",
    "Each key should be a single string where the words that form the history have been concatenated using spaces. Given a key, its corresponding value should be a dictionary where each word type in the vocabulary is associated with its probability of appearing after the key. For example, the entry for the history 'w1 w2' should look like:\n",
    "\n",
    "    \n",
    "    lm['w1 w2'] = {'w0': 0.001, 'w1' : 1e-6, 'w2' : 1e-6, 'w3': 0.003, ...}\n",
    "    \n",
    "In this example, we also want to store `lm['w2']` and `lm['']`, which contain the bigram and unigram distributions respectively.\n",
    "\n",
    "*Hint:* You might find the **defaultdict** and **Counter** classes in the **collections** module to be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19adcd85-9e8e-4aae-b32b-05029a953dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _backoff(lm: list[str], h: str) -> str:\n",
    "    \"\"\"\n",
    "    Backoff until we find a history in lm or reach an empty string.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lm : str\n",
    "        The trained n-gram language model.\n",
    "    h : str\n",
    "        The history string (context) to look up in the model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The longest found history in lm, or an empty string if no match is found.\n",
    "    \"\"\"\n",
    "    \n",
    "    while h not in lm and h:\n",
    "            h = \" \".join(h.split()[:-1]) # backoff n-1\n",
    "    return h\n",
    "\n",
    "def train_ngram_lm(data: list[str], n: int = 3) -> dict[str, dict[str, float]]:\n",
    "    \"\"\" \n",
    "    Trains an n-gram language model with backoff support.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : list of str\n",
    "        A list of tokens representing the training dataset.\n",
    "    n : int, optional\n",
    "        The order of the n-gram model. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict of {str: dict of {str: float}}\n",
    "        A dictionary where:\n",
    "        - Keys are space-separated strings representing the history (n - 1 context).\n",
    "        - Values are dictionaries mapping possible next words to their probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    n -= 1\n",
    "    data = (['<S>'] * n) + data\n",
    "    lm = defaultdict(Counter)\n",
    "\n",
    "    # collect n..0-grams\n",
    "    for i in range(len(data) - n): # up to len(data - n)\n",
    "        for j in range (n, -1, -1):\n",
    "\n",
    "            h, w = ' '.join(data[i: i + j]), data[i + n] \n",
    "            lm[h][w] += 1\n",
    "\n",
    "    # convert raw counts into MLEs\n",
    "    lm = { \n",
    "        h : {w: count / sum(word_counts.values()) for w, count in word_counts.items()} \n",
    "        for h, word_counts in lm.items() \n",
    "    }\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a035c635-bddb-480b-940a-fc38a0482b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ngram_lm():\n",
    "    import random\n",
    "    global DATA\n",
    "  \n",
    "    print('checking empty history ...')\n",
    "    lm1 = train_ngram_lm(DATA['train'], n = 1)\n",
    "    assert '' in lm1, \"empty history should be in the language model!\"\n",
    "    \n",
    "    print('checking probability distributions ...')\n",
    "    lm2 = train_ngram_lm(DATA['train'], n = 2 )\n",
    "    sample = [sum(lm2[k].values()) for k in random.sample(list(lm2), 10)]\n",
    "    assert all([a > 0.999 and a < 1.001 for a in sample]), \"lm[history][word] should sum to 1!\"\n",
    "    \n",
    "    print('checking lengths of histories ...')\n",
    "    lm3 = train_ngram_lm(DATA['train'], n = 3)\n",
    "    assert len(set([len(k.split()) for k in list(lm3)])) == 3, \"lm object should store histories of all sizes!\"\n",
    "    \n",
    "    print('checking word distribution values ...')\n",
    "    assert lm1['']['the'] < 0.064 and lm1['']['the'] > 0.062 and \\\n",
    "           lm2['the']['first'] < 0.017 and lm2['the']['first'] > 0.016 and \\\n",
    "           lm3['the first']['time'] < 0.106 and lm3['the first']['time'] > 0.105, \\\n",
    "           \"values do not match!\"\n",
    "    \n",
    "    print(\"Congratulations, you passed the ngram check!\")\n",
    "    \n",
    "test_ngram_lm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c298d5-e036-4a1e-953c-a3888336b8bc",
   "metadata": {},
   "source": [
    "## Q2: Generate text from n-gram language model (40 pts)\n",
    "\n",
    "Complete the following `generate_text` function based on these input/output requirements:\n",
    "\n",
    "*Input:*\n",
    "\n",
    "+ **lm**: the lm object is the dictionary you return from  the **train_ngram_lm** function\n",
    "+ **vocab**: vocab is a list of unique word types in the training set, already computed for you during data loading.\n",
    "+ **context**: the input context string that you want to condition your language model on, should be a space-separated string of tokens\n",
    "+ **order**: order of your language model (i.e., \"n\" in the \"n-gram\" model)\n",
    "+ **num_tok**: number of tokens to be generated following the input context\n",
    "\n",
    "\n",
    "*Output:*\n",
    "\n",
    "+ generated text, should be a space-separated string\n",
    "    \n",
    "*Hint:*\n",
    "\n",
    "After getting the next-word distribution given history, try using **[numpy.random.choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html)** to sample the next word from the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9833b768-b01c-4f4e-9bca-70019f18b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(lm: dict[str, dict[str, float]], vocab: list[str], context: str = \"he is the\", n: int = 3, n_tokens: int = 25) -> str:\n",
    "    \"\"\"\n",
    "    Generates text using an n-gram language model with backoff support.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lm : dict[str, dict[str, float]]\n",
    "        The trained n-gram language model where:\n",
    "        - Keys are space-separated history strings (n-1 context).\n",
    "        - Values are dictionaries mapping next words to their probabilities.\n",
    "    vocab : list[str]\n",
    "        A list of unique words from the training dataset, used for fallback sampling.\n",
    "    context : str, optional\n",
    "        The initial text to seed the text generation process. Defaults to \"he is the\".\n",
    "    n : int, optional\n",
    "        The order of the n-gram model. Defaults to 3 (trigram model).\n",
    "    n_tokens : int, optional\n",
    "        The number of tokens to generate. Defaults to 25.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A space-separated string representing the generated text.\n",
    "    \"\"\"\n",
    "\n",
    "    n -= 1\n",
    "    history = context.split()[:n]\n",
    "    out = context.split()\n",
    "    \n",
    "    for _ in range(n_tokens):\n",
    "        h = _backoff(lm, \" \".join(history)) # backoff (n - 1) if history isn't found in lm\n",
    "\n",
    "        # convert dict keys and values to arrays for np.random.choice sampling\n",
    "        if h in lm:\n",
    "            a, p = zip(*lm[h].items()) # w = [...] p = [...]\n",
    "            w = np.random.choice(a = a, p = p)\n",
    "        else:\n",
    "            w = np.random.choice(a = vocab) # fallback to random selection\n",
    "         \n",
    "        out.append(w) # append to growing sequence \n",
    "        history = (history + [w])[-n:]\n",
    "    return \" \".join(out) # return full generated text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fbab80-83be-4789-833e-ad77eb8b06d5",
   "metadata": {},
   "source": [
    "Now try to generate some texts, generated by ngram language model with different orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e4f974-09f0-4fea-9067-7dba785dc0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "global DATA, VOCAB\n",
    "order = 1\n",
    "print(f\"[{order}-GRAM]: {generate_text(lm = train_ngram_lm(data = DATA['train'], n = order), vocab = VOCAB, context = 'he is the', n = order)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21de76f-4864-439d-97a1-d78d250d7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "global DATA, VOCAB\n",
    "order = 2\n",
    "print(f\"[{order}-GRAM]: {generate_text(lm = train_ngram_lm(data = DATA['train'], n = order), vocab = VOCAB, context = 'he is the', n = order)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac1484f-00bb-4519-9134-5b447b68b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "global DATA, VOCAB\n",
    "order = 3\n",
    "print(f\"[{order}-GRAM]: {generate_text(lm = train_ngram_lm(data = DATA['train'], n = order), vocab = VOCAB, context = 'he is the', n = order)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ee800-b881-4173-8c95-525213b6136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "global DATA, VOCAB\n",
    "order = 4\n",
    "print(f\"[{order}-GRAM]: {generate_text(lm = train_ngram_lm(data = DATA['train'], n = order), vocab = VOCAB, context='he is the', n = order)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe32855-37be-494e-bb66-b96bdc6b5f5d",
   "metadata": {},
   "source": [
    "## Q3 : Evaluate the models (10 pts)\n",
    "Now let's evaluate the models quantitively using the intrinsic metric **perplexity**. \n",
    "\n",
    "Recall perplexity is the inverse probability of the test text\n",
    "$$\\text{PP}(w_1, \\dots, w_t) = P(w_1, \\dots, w_t)^{-\\frac{1}{T}}$$\n",
    "\n",
    "For an n-gram model, perplexity is computed by\n",
    "$$\\text{PP}(w_1, \\dots, w_t) = \\left[\\prod_{t=1}^T P(w_t|w_{t-1},\\ldots,w_{t-n+1})\\right]^{-\\frac{1}{T}}$$\n",
    "\n",
    "To address the numerical issue (underflow), we usually compute\n",
    "$$\\text{PP}(w_1, \\dots, w_t) = \\exp\\left(-\\frac{1}{T}\\sum_i \\log P(w_t|w_{t-1},\\ldots,w_{t-n+1})\\right)$$\n",
    "\n",
    "\n",
    "*Input:*\n",
    "\n",
    "+ **lm**: the language model you trained (the object you returned from the `train_ngram_lm` function)\n",
    "+ **data**: test data\n",
    "+ **vocab**: the list of unique word types in the training set\n",
    "+ **order**: order of the lm\n",
    "\n",
    "*Output:*\n",
    "\n",
    "+ the perplexity of test data\n",
    "\n",
    "*Hint:*\n",
    "\n",
    "+ If the history is not in the **lm** object, back-off to (n-1) order history to check if it is in **lm**. If no history can be found, just use `1/|V|` where `|V|` is the size of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87e412a7-2ad8-4a79-a4b5-31bc81295e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(lm: dict[str, dict[str, float]], data: list[str], vocab: list[str], n: int = 3) -> float:\n",
    "    \"\"\"\n",
    "    Computes perplexity using backoff logic for an n-gram language model.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    lm  : dict of {str: dict of {str: float}}\n",
    "        The language model returned by train_ngram_lm (with backoff counts).\n",
    "        - Keys are space-separated history strings (n - 1 context).\n",
    "        - Values are dictionaries mapping next words to probabilities.\n",
    "    data  : list of tokens (test data).\n",
    "    vocab : list of unique tokens from training.\n",
    "    n     : int, the original n (e.g. 3 => trigram). We'll internally do (n -= 1).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    perplexity : float\n",
    "        The perplexity of 'data' under the language model 'lm'.\n",
    "    \"\"\"\n",
    "\n",
    "    n -= 1  # pad\n",
    "    data = (['<S>'] * n) + data\n",
    "    T = 0 # accumulator for token count\n",
    "    log_sum = 0.0 \n",
    "\n",
    "    for i in range(len(data) - n):\n",
    "        h, w = _backoff(lm, ' '.join(data[i : i + n])), data[i + n] # backoff (n - 1)\n",
    "\n",
    "        # probability lookup\n",
    "        prob = lm[h][w] if h in lm and w in lm[h] else 1.0 / len(vocab) # fallback to 1 / |vocab|\n",
    "        log_sum += log(prob) # accumulate log prob\n",
    "        T += 1\n",
    "\n",
    "    # perplexity = exp( - (1/T) * sum of log probs )\n",
    "    ppl = exp(-log_sum / T)\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02749daa-96b5-4f50-aa1c-0279a6e24cef",
   "metadata": {},
   "source": [
    "Let's evaluate the language model with different orders. You should see a decrease in perplexity as the order increases. As a reference, the perplexity of the unigram, bigram, trigram, and 4-gram language models should be around 795, 203, 141, and 130 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5293167-78a8-49c6-93dd-cbfdff8c3212",
   "metadata": {},
   "outputs": [],
   "source": [
    "global DATA, VOCAB\n",
    "\n",
    "for o in [1, 2, 3, 4]:\n",
    "    lm = train_ngram_lm(DATA['train'], n = o)\n",
    "    print('order {} ppl {}'.format(o, compute_perplexity(lm, DATA['test'], VOCAB, n = o)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
